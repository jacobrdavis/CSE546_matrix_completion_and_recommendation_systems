{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobrdavis/CSE546_matrix_completion_and_recommendation_systems/blob/main/matrix_completion_and_recommendation_systems_started_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Completion and Recommendation System\n",
        "\n",
        "In this problem, we will use the 100K MovieLens dataset available at https://grouplens.org/datasets/movielens/100k/ to estimate unknown user ratings given their previous ratings. \n",
        "\n",
        "Create a copy of this notebook on your personal drive as started code. Download the dataset and upload ``u.data`` under the \"Files\" tab in the sidebar (**NOTE:** you will need to re-upload this file if you disconnect and delete the current runtime). "
      ],
      "metadata": {
        "id": "ofxrSTrclA0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO1zQE45kza0"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from scipy.sparse.linalg import svds\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the 100K MovieLens data."
      ],
      "metadata": {
        "id": "9ZNgGlPOnVZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "with open('u.data') as csvfile:\n",
        "    spamreader = csv.reader(csvfile, delimiter='\\t')\n",
        "    for row in spamreader:\n",
        "        data.append([int(row[0])-1, int(row[1])-1, int(row[2])])\n",
        "data = np.array(data)\n",
        "\n",
        "num_observations = len(data)  # num_observations = 100,000\n",
        "num_users = max(data[:,0])+1  # num_users = 943, indexed 0,...,942\n",
        "num_items = max(data[:,1])+1  # num_items = 1682 indexed 0,...,1681\n",
        "\n",
        "np.random.seed(1)\n",
        "num_train = int(0.8*num_observations)\n",
        "perm = np.random.permutation(data.shape[0])\n",
        "train = data[perm[0:num_train],:]\n",
        "test = data[perm[num_train::],:]\n",
        "\n",
        "print(f\"Successfully loaded 100K MovieLens dataset with\",\n",
        "      f\"{len(train)} training samples and {len(test)} test samples\")"
      ],
      "metadata": {
        "id": "pv_V_YC7nU6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8775950b-a3ff-476e-e5ef-eeccb4e25204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 100K MovieLens dataset with 80000 training samples and 20000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part a\n",
        "Our first estimator pools all users together and, for each movie, outputs as its prediction the average user rating of that movie in ``train``. That is, if $\\mu \\in \\mathbb{R}^m$ is a vector where $\\mu_i$ is the average rating of the users that rated the $i$-th movie. Write this estimator $\\widehat{R}$ as a rank-one matrix. \n",
        "\n",
        "Compute the estimate $\\widehat{R}$. What is $\\mathcal{E}_{\\rm test} (\\widehat{R})$ for this estimate?"
      ],
      "metadata": {
        "id": "2mkeYWwdniAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy3n5ljTkza3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa277f2-c839-4e3a-c879-08b80512a64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1680\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here\n",
        "# Compute estimate\n",
        "print(np.max(train[:, 1]))\n",
        "\n",
        "# Evaluate test error\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part b\n",
        "Create a matrix $\\widetilde{R}_{i, j} \\in \\mathbb{R}^{m \\times n}$ and set its entries equal to the known values in the training set, and $0$ otherwise. \n",
        "\n",
        "Let $\\widehat{R}^{(d)}$ be the best rank-$d$ approximation (in terms of squared error) approximation to $\\widetilde{R}$. This is equivalent to computing the singular value decomposition (SVD) and using the top $d$ singular values. This learns a lower-dimensional vector representation for users and movies, assuming that each user would give a rating of $0$ to any movie they have not reviewed.\n",
        "\n",
        "- For each $d = 1, 2, 5, 10, 20, 50$, compute the estimator $\\widehat{R}^{(d)}$. We recommend using an efficient solver, such as ``scipy.sparse.linalg.svds``.\n",
        "- Plot the average squared error of predictions on the training set and test set on a single plot, as a function of $d$."
      ],
      "metadata": {
        "id": "y7BoyoWHpl2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6t87ACNkza5"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "# Create the matrix R twiddle (\\widetilde{R}).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWf-Umhdkza5"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "def construct_estimator(d, r_twiddle):\n",
        "  raise NotImplementedError(\"Your code goes here\")\n",
        "\n",
        "\n",
        "def get_error(d, r_twiddle, dataset):\n",
        "  raise NotImplementedError(\"Your code goes here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6uMbMhGkza6"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "# Evaluate train and test error for: d = 1, 2, 5, 10, 20, 50.\n",
        "\n",
        "\n",
        "# Plot both train and test error as a function of d on the same plot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part c\n",
        "Replacing all missing values by a constant may impose strong and potentially incorrect assumptions on the unobserved entries of $R$. A more reasonable choice is to minimize the mean squared error (MSE) only on rated movies. Define a loss function:\n",
        "$$\n",
        "\\mathcal{L} \\left( \\{u_i\\}_{i=1}^m, \\{v_j\\}_{j=1}^n \\right) :=\n",
        "\\sum_{(i, j, R_{i, j}) \\in {\\rm train}} (\\langle u_i,v_j\\rangle - R_{i,j})^2 + \n",
        "\\lambda \\sum_{i=1}^m \\|u_i\\|_2^2 +\n",
        "\\lambda \\sum_{j=1}^n \\|v_j\\|_2^2\n",
        "$$\n",
        "where $\\lambda > 0$ is the regularization coefficient. We will implement algorithms to learn vector representations by minimizing the above loss. You may need to tune $\\lambda$ and $\\sigma$ to optimize the loss.\n",
        "\n",
        "Implement alternating minimization (as defined in the homework spec) and plot the MSE of ``train`` and ``test`` for $d \\in \\{1, 2, 5, 10, 20, 50\\}$."
      ],
      "metadata": {
        "id": "W8x30SANrqN4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1Jt9wLzkza6"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "def closed_form_u(V, U, l):\n",
        "  raise NotImplementedError(\"Your code goes here\")\n",
        "\n",
        "\n",
        "def closed_form_v(V, U, l):\n",
        "  raise NotImplementedError(\"Your code goes here\")\n",
        "\n",
        "\n",
        "def construct_alternating_estimator(\n",
        "    d, r_twiddle, l=0.0, delta=1e-5, sigma=0.1, U=None, V=None\n",
        "):\n",
        "  raise NotImplementedError(\"Your code goes here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4SLeTASkza8"
      },
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "# Evaluate train and test error for: d = 1, 2, 5, 10, 20, 50.\n",
        "\n",
        "\n",
        "# Plot both train and test error as a function of d on the same plot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part d (Extra credit)\n",
        "Implement any algorithm you'd like (you must implement it yourself; do not use an off-the-shelf algorithm from e.g. ``scikit-learn``) to find an estimator that achieves a test error of less than 0.9.\n",
        "\n",
        "**NOTE:** This is extra credit. Please do not start unless you have finished all other parts of this homework!"
      ],
      "metadata": {
        "id": "i8rpr8fcvY0h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf2PWF5Rkza-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}